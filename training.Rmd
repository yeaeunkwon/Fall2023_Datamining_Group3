---
title: "weather-forecast"
author: "Group 3"
date: "2023-11-30"
output: html_document
---

```{r load-packages, message = FALSE}
if (!require(pacman))
  install.packages(pacman)

pacman::p_load(forcats,tidyverse, dplyr, ggplot2, keras, tensorflow, caret, reticulate)
```

```{r warning=FALSE}
#| label: load dataset
data <- read_csv("data/maindata.csv")
#view(data)

monthly_data <- read_csv("data/data_monthly.csv")
view(monthly_data)
#head(monthly_data)
```


```{r}
#| label: data preprocessing
# Preprocess the data: Remove NAs and skip unwanted columns
#cleaned_data <- na.omit(monthly_data)  # Remove rows with NAs
cleaned_data <- monthly_data[, !names(monthly_data) %in% c("...1","tempmax", "tempmin", "feelslikemax","feelslikemin","solarradiation", "solarenergy", "uvindex", "severerisk", "snowdepth", "snow" )]  # remove few columns

view(cleaned_data)

# Replace NAs with 0 (if needed)
cleaned_data[is.na(cleaned_data)] <- 0

glimpse(cleaned_data)
```


```{r}
#| label: data Normalization
# Normalize the data (scaling to a range of [0,1])
normalize_col <- c("winddir", "sealevelpressure", "sunset_time") 

cleaned_data[normalize_col] <- scale(cleaned_data[normalize_col])
view(cleaned_data)
```


```{r}
#| label: Split data into test, train and validation
# Split the data into train, validation, and test sets
split <- sample(1:nrow(cleaned_data), size = 0.7 * nrow(cleaned_data))
train_data <- cleaned_data[split, ]
test_val_data <- cleaned_data[-split, ]
split_val <- sample(1:nrow(test_val_data), size = 0.5 * nrow(test_val_data))
validation_data <- test_val_data[split_val, ]
test_data <- test_val_data[-split_val, ]

#train data
glimpse(train_data)
#test data
glimpse(test_data)
#validation data
glimpse(validation_data)

```


```{r}
#| label: CNN- Convolutional Neural Network model
#install.packages("reticulate")
# Define input shape based on the number of features in your data
input_shape <- ncol(train_data) - 1 # Excluding the target variable 'temp'

# Define the NN architecture
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = input_shape) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'linear') # Linear activation for regression

summary(model)

```


```{r}
# Compile the model
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
```


```{r}
# Compile the model using legacy adam optimizer
optimizer <- keras$optimizers$legacy$Adam(learning_rate = 0.001)
model$compile(optimizer = optimizer, loss = "mean_squared_error", metrics = c("mean_absolute_error"))

```


```{r}
# Train the model
history <- model %>%
  fit(
    as.matrix(train_data[, -which(names(train_data) == "temp")]),  # Extract features except 'temp'
    train_data$temp,  # Target variable 'temp'
    epochs = 400,
    batch_size = 32,
    validation_data = list(
      as.matrix(validation_data[, -which(names(validation_data) == "temp")]),
      validation_data$temp
    ),
    verbose = 1  # Set verbosity to 1 for progress display
  )


print(history)
plot(history)



# Get MAE scores
train_mae <- history$metrics$val_mean_absolute_error[[length(history$metrics$val_mean_absolute_error)]]
val_mae <- history$metrics$mean_absolute_error[[length(history$metrics$mean_absolute_error)]]



# Print MAE scores
cat("Train MAE:", train_mae, "\n")
cat("Validation MAE:", val_mae, "\n")


# Predict on test data
predictions <- model %>% predict(as.matrix(test_data[, -which(names(test_data) == "temp")]))
print("Predicted values:")
print(predictions)
write.csv(predictions, "predictions_400e32b.csv")

# Evaluate model on test data
evaluation <- model %>% evaluate(as.matrix(test_data[, -which(names(test_data) == "temp")]), test_data$temp)
print("Evaluation metrics on test data:")
print(evaluation)
write.csv(evaluation, "evaluation_400e32b.csv")

```
while performing regression(predicting temperature in this case), we use metrics like Mean Absolute Error (MAE) or Mean Squared Error (MSE) which are more relevant for evaluating the performance of the model as accuracy is not an appropriate metric for regression tasks.
Mean Absolute Error (MAE) : This measures the average absolute difference between the predicted values and the actual values in the dataset.Lower values indicate a better fit of the model to the data.















